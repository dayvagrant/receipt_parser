{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create the first baseline model to classify categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "from typing import Optional, Union, Dict, Any, Tuple, List\n",
    "from receipt_parser import RuleBased\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import youtokentome as yttm\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "import random\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryDataset(Dataset):\n",
    "    def __init__(self, cat_df: pd.DataFrame, vocab_size: int = 500, use_padding: bool = False, chunk_length: int = 50, pad_idx: int = 0):\n",
    "        self.use_padding = use_padding\n",
    "        self.chunk_length = chunk_length\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "        # Prepare DataFrame\n",
    "        self.le = LabelEncoder()\n",
    "        self.cat_df = self.__transfrom_df(cat_df)\n",
    "        self.train, self.val, self.test = self.split_df(self.cat_df)\n",
    "        self._lookup_dict = {'train': self.train,\n",
    "                             'val': self.val,\n",
    "                             'test': self.test}\n",
    "        \n",
    "        # Train BPE model:\n",
    "        self.vocab_size = vocab_size\n",
    "        self.path_to_bpe = 'data_cat/train_bpe_model.yttm'\n",
    "        self.path_to_train = 'data_cat/train.txt'\n",
    "        self.save_texts_to_file(self.train['name_norm'], self.path_to_train)\n",
    "        self.tokenizer = self.build_bpe_model()\n",
    "        \n",
    "        self.set_split('train')\n",
    "    \n",
    "    def set_split(self, split=\"train\"):\n",
    "        \"\"\"\n",
    "        Selects the splits in the dataset.\n",
    "        split (str): one of \"train\", \"val\", or \"test\"\n",
    "        \"\"\"\n",
    "\n",
    "        self._target_df = self._lookup_dict[split]\n",
    "    \n",
    "    def build_bpe_model(self) -> yttm.BPE:\n",
    "        yttm.BPE.train(\n",
    "            data=self.path_to_train,\n",
    "            vocab_size=self.vocab_size,\n",
    "            model=self.path_to_bpe,\n",
    "            pad_id=self.pad_idx\n",
    "        )\n",
    "        return yttm.BPE(self.path_to_bpe)\n",
    "    \n",
    "    @staticmethod\n",
    "    def ensure_length(txt: List[int], out_len: int, pad_value: int) -> List[int]:\n",
    "        \"\"\"Add PAD-indices to a `out_len` length.\"\"\"\n",
    "\n",
    "        if len(txt) < out_len:\n",
    "            txt = list(txt) + [pad_value] * (out_len - len(txt))\n",
    "        else:\n",
    "            txt = txt[:out_len]\n",
    "        return txt\n",
    "    \n",
    "    def __transfrom_df(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Drop duplicates and NAN objects. Rename `category` column.\n",
    "        Encode category to idx using `LabelEncoder`.\n",
    "        \"\"\"\n",
    "        \n",
    "        \n",
    "        df = df.drop_duplicates('name_norm').dropna()\n",
    "        df = df.rename(columns = {'Категория': 'category'})\n",
    "        df['target'] = self.le.fit_transform(df['category'])\n",
    "        return df\n",
    "\n",
    "    @staticmethod\n",
    "    def save_texts_to_file(texts: pd.Series, out_file: str) -> None:\n",
    "        \"\"\"Save text to .txt fromat for BPE model.\"\"\"\n",
    "        \n",
    "        with open(out_file, 'w') as outf:\n",
    "            outf.write('\\n'.join(texts))\n",
    "    \n",
    "    @staticmethod\n",
    "    def split_df(df: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        60% - train set,\n",
    "        20% - validation set,\n",
    "        20% - test set\n",
    "        \n",
    "        Return train, validation, test.\n",
    "        \"\"\"\n",
    "\n",
    "        return np.split(df.sample(frac=1), [int(.6*len(df)), int(.8*len(df))])\n",
    "    \n",
    "    def get_labels(self) -> List[str]:\n",
    "        \"\"\"Return all unique categories in dataframe.\"\"\"\n",
    "\n",
    "        return self.train['category'].unique()\n",
    "    \n",
    "    def decode_target(self, target: int) -> str:\n",
    "        return self.le.inverse_transform([target])[0]\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "        return len(self._target_df)\n",
    "\n",
    "    def __getitem__(self, index: int) -> Dict[str, Any]:\n",
    "        row = self._target_df.iloc[index]\n",
    "        cat_vector = self.tokenizer.encode(row.name_norm)\n",
    "        if self.use_padding:\n",
    "            cat_vector = self.ensure_length(cat_vector, self.chunk_length, self.pad_idx)\n",
    "        cat_vector = torch.tensor(cat_vector)\n",
    "        target = row.target\n",
    "        \n",
    "        return {'x_data': cat_vector,\n",
    "                'y_target': target}\n",
    "    \n",
    "\n",
    "def generate_batch(batch):\n",
    "    label = torch.tensor([entry['y_target'] for entry in batch])\n",
    "    text = [entry['x_data'] for entry in batch]\n",
    "    offsets = [0] + [len(entry) for entry in text]\n",
    "    # torch.Tensor.cumsum returns the cumulative sum\n",
    "    # of elements in the dimension dim.\n",
    "\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text = torch.cat(text)\n",
    "    \n",
    "    return text, offsets, label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CategoryClassifier(nn.Module):\n",
    "    \"\"\"A simple perceptron baseline moedel.\"\"\"\n",
    "\n",
    "    def __init__(self, vocab_size: int, embed_dim: int, num_class: int, pad_idx: int = 0):\n",
    "        super(CategoryClassifier, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim)\n",
    "#         self.embedding = nn.Embedding(vocab_size, embedding_dim=embed_dim, padding_idx=pad_idx)\n",
    "        self.fc = nn.Linear(in_features=embed_dim, \n",
    "                             out_features=num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "        \n",
    "    def forward(self, x_in, offsets, apply_sigmoid=False):\n",
    "        \"\"\"The forward pass of the classifier.\"\"\"\n",
    "        \n",
    "        embedded = self.embedding(x_in, offsets)\n",
    "        y_out = self.fc(embedded)\n",
    "        if apply_sigmoid:\n",
    "            y_out = torch.sigmoid(y_out)\n",
    "        return y_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 500\n",
    "USE_PADDING = False\n",
    "CHUNK_LENGHT = 50\n",
    "PAD_IDX = 0\n",
    "\n",
    "# Init dataset:\n",
    "df = pd.read_csv('train_cat.csv')\n",
    "df.shape\n",
    "dataset = CategoryDataset(df, VOCAB_SIZE, USE_PADDING, CHUNK_LENGHT, PAD_IDX)\n",
    "\n",
    "# Init model:\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "NUN_CLASS = len(dataset.get_labels())\n",
    "EMBED_DIM = 50\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "model = CategoryClassifier(VOCAB_SIZE, EMBED_DIM, NUN_CLASS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0675(train)\t|\tAcc: 39.5%(train)\n",
      "\tLoss: 0.0003(valid)\t|\tAcc: 58.8%(valid)\n",
      "Epoch: 2  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0414(train)\t|\tAcc: 65.1%(train)\n",
      "\tLoss: 0.0002(valid)\t|\tAcc: 68.4%(valid)\n",
      "Epoch: 3  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0329(train)\t|\tAcc: 71.1%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 71.9%(valid)\n",
      "Epoch: 4  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0290(train)\t|\tAcc: 73.7%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 73.8%(valid)\n",
      "Epoch: 5  | time in 0 minutes, 13 seconds\n",
      "\tLoss: 0.0267(train)\t|\tAcc: 75.3%(train)\n",
      "\tLoss: 0.0001(valid)\t|\tAcc: 74.4%(valid)\n"
     ]
    }
   ],
   "source": [
    "def transform_target(target):\n",
    "    res = []\n",
    "    for i in target:\n",
    "        t_ = np.zeros(NUN_CLASS)\n",
    "        t_[i] = 1\n",
    "        res.append(t_)\n",
    "    return res\n",
    "\n",
    "import time\n",
    "\n",
    "N_EPOCHS = 5\n",
    "model = model.to(device)\n",
    "loss_func = nn.CrossEntropyLoss().to(device)\n",
    "\n",
    "learning_rate=0.001\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer=optimizer,\n",
    "                                                 mode='min', factor=0.5,\n",
    "                                                 patience=1)\n",
    "\n",
    "# optimizer = torch.optim.SGD(model.parameters(), lr=4.0)\n",
    "# scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1, gamma=0.9)\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    \n",
    "    # Train the model\n",
    "    dataset.set_split('train')\n",
    "    train_loss = 0\n",
    "    train_acc = 0\n",
    "    model.train()\n",
    "    data = DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True,\n",
    "                      collate_fn=generate_batch)\n",
    "    for i, (text, offsets, target) in enumerate(data):\n",
    "        optimizer.zero_grad()\n",
    "        text, offsets, target = text.to(device), offsets.to(device), target.to(device)\n",
    "        output = model(text, offsets)\n",
    "#         target = torch.tensor(transform_target(target)).long().to(device)\n",
    "        loss = loss_func(output, target)\n",
    "        train_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_acc += (output.argmax(1) == target).sum().item()\n",
    "\n",
    "    # Adjust the learning rate\n",
    "    train_loss, train_acc = train_loss / len(dataset), train_acc / len(dataset)\n",
    "    scheduler.step(train_loss)\n",
    "    \n",
    "    # Eval the model\n",
    "    dataset.set_split('val')\n",
    "    loss = 0\n",
    "    acc = 0\n",
    "    model.eval()\n",
    "    data = DataLoader(dataset, batch_size=BATCH_SIZE, collate_fn=generate_batch)\n",
    "    \n",
    "    for i, (text, offsets, target) in enumerate(data):\n",
    "        text, offsets, target = text.to(device), offsets.to(device), target.to(device)\n",
    "#         target = torch.tensor(transform_target(target)).long().to(device)\n",
    "        output = model(text, offsets)\n",
    "        loss = loss_func(output, target)\n",
    "        loss += loss.item()\n",
    "        acc += (output.argmax(1) == target).sum().item()\n",
    "    \n",
    "    valid_loss, valid_acc = loss / len(dataset), acc / len(dataset)\n",
    "    \n",
    "    \n",
    "    secs = int(time.time() - start_time)\n",
    "    mins = secs / 60\n",
    "    secs = secs % 60\n",
    "\n",
    "    print('Epoch: %d' %(epoch + 1), \" | time in %d minutes, %d seconds\" %(mins, secs))\n",
    "    print(f'\\tLoss: {train_loss:.4f}(train)\\t|\\tAcc: {train_acc * 100:.1f}%(train)')\n",
    "    print(f'\\tLoss: {valid_loss:.4f}(valid)\\t|\\tAcc: {valid_acc * 100:.1f}%(valid)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(name_norm: str) -> str:\n",
    "    with torch.no_grad():\n",
    "        text = dataset.tokenizer.encode(name_norm)\n",
    "        text = torch.tensor(text).to(device)\n",
    "        output = model(text, torch.tensor([0]).to(device))\n",
    "        return dataset.decode_target(output.argmax(1).item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name_norm</th>\n",
       "      <th>category</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27600</th>\n",
       "      <td>биойогурт питьевой  черная смородина</td>\n",
       "      <td>Молоко, сыр, яйца</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29972</th>\n",
       "      <td>зефир \"сладкие истории\" вкусом крем брюле негл...</td>\n",
       "      <td>Хлеб, сладости, снеки</td>\n",
       "      <td>19</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27110</th>\n",
       "      <td>зубная паста  морские минералы</td>\n",
       "      <td>Красота, гигиена, бытовая химия</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21664</th>\n",
       "      <td>джем крыжовниковый</td>\n",
       "      <td>Соусы, орехи, консервы</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9461</th>\n",
       "      <td>колбаса охотничья сырокопченая</td>\n",
       "      <td>Птица, мясо, деликатесы</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28063</th>\n",
       "      <td>корм собак вкусные потрошки говядина сердце</td>\n",
       "      <td>Зоотовары</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24537</th>\n",
       "      <td>хлопья   микс органические</td>\n",
       "      <td>Макароны, крупы, специи</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16145</th>\n",
       "      <td>краска волос  карамель</td>\n",
       "      <td>Красота, гигиена, бытовая химия</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22289</th>\n",
       "      <td>био йогурт питьевой злаками</td>\n",
       "      <td>Молоко, сыр, яйца</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44658</th>\n",
       "      <td>подарочный набор совершенство</td>\n",
       "      <td>Красота, гигиена, бытовая химия</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               name_norm  \\\n",
       "27600               биойогурт питьевой  черная смородина   \n",
       "29972  зефир \"сладкие истории\" вкусом крем брюле негл...   \n",
       "27110                     зубная паста  морские минералы   \n",
       "21664                                 джем крыжовниковый   \n",
       "9461                      колбаса охотничья сырокопченая   \n",
       "28063        корм собак вкусные потрошки говядина сердце   \n",
       "24537                         хлопья   микс органические   \n",
       "16145                             краска волос  карамель   \n",
       "22289                        био йогурт питьевой злаками   \n",
       "44658                      подарочный набор совершенство   \n",
       "\n",
       "                              category  target  \n",
       "27600                Молоко, сыр, яйца       9  \n",
       "29972            Хлеб, сладости, снеки      19  \n",
       "27110  Красота, гигиена, бытовая химия       7  \n",
       "21664           Соусы, орехи, консервы      16  \n",
       "9461           Птица, мясо, деликатесы      14  \n",
       "28063                        Зоотовары       6  \n",
       "24537          Макароны, крупы, специи       8  \n",
       "16145  Красота, гигиена, бытовая химия       7  \n",
       "22289                Молоко, сыр, яйца       9  \n",
       "44658  Красота, гигиена, бытовая химия       7  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tmp = dataset.test.sample(10)\n",
    "tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "биойогурт питьевой  черная смородина --> Молоко, сыр, яйца\n",
      "зефир \"сладкие истории\" вкусом крем брюле неглазированный --> Хлеб, сладости, снеки\n",
      "зубная паста  морские минералы --> Красота, гигиена, бытовая химия\n",
      "джем крыжовниковый --> Товары для дома и дачи\n",
      "колбаса охотничья сырокопченая --> Птица, мясо, деликатесы\n",
      "корм собак вкусные потрошки говядина сердце --> Зоотовары\n",
      "хлопья   микс органические --> Макароны, крупы, специи\n",
      "краска волос  карамель --> Красота, гигиена, бытовая химия\n",
      "био йогурт питьевой злаками --> Молоко, сыр, яйца\n",
      "подарочный набор совершенство --> Красота, гигиена, бытовая химия\n"
     ]
    }
   ],
   "source": [
    "for name, pred in zip(tmp['name_norm'], tmp['name_norm'].apply(predict)):\n",
    "    print(f'{name} --> {pred}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['Алкоголь', 'Бытовая техника', 'Воды, соки, напитки',\n",
       "       'Дача и гриль', 'Другое', 'Замороженные продукты', 'Зоотовары',\n",
       "       'Красота, гигиена, бытовая химия', 'Макароны, крупы, специи',\n",
       "       'Молоко, сыр, яйца', 'Овощи, фрукты, ягоды',\n",
       "       'Подборки и готовые блюда', 'Постные продукты', 'Посуда',\n",
       "       'Птица, мясо, деликатесы', 'Рыба, икра', 'Соусы, орехи, консервы',\n",
       "       'Товары для дома и дачи', 'Товары для мам и детей',\n",
       "       'Хлеб, сладости, снеки', 'Чай, кофе, сахар'], dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.le.classes_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), 'receipt_parser/models/baseline_model.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictCategory:\n",
    "    def __init__(self, path_to_bpe: str, path_to_model: str, model_params: Dict[str, int]):\n",
    "        self.bpe_model = yttm.BPE(path_to_bpe)\n",
    "        self.categories: List[str] = [\n",
    "            'Алкоголь', 'Бытовая техника', 'Воды, соки, напитки',\n",
    "           'Дача и гриль', 'Другое', 'Замороженные продукты', 'Зоотовары',\n",
    "           'Красота, гигиена, бытовая химия', 'Макароны, крупы, специи',\n",
    "           'Молоко, сыр, яйца', 'Овощи, фрукты, ягоды',\n",
    "           'Подборки и готовые блюда', 'Постные продукты', 'Посуда',\n",
    "           'Птица, мясо, деликатесы', 'Рыба, икра', 'Соусы, орехи, консервы',\n",
    "           'Товары для дома и дачи', 'Товары для мам и детей',\n",
    "           'Хлеб, сладости, снеки', 'Чай, кофе, сахар'\n",
    "        ]\n",
    "        self.device = torch.device(\"cpu\")\n",
    "        self.model = CategoryClassifier(**model_params)\n",
    "        self.model.load_state_dict(torch.load(path_to_model, map_location=self.device))\n",
    "        self.model.eval()\n",
    "        \n",
    "    def predict(self, name_norm: str) -> str:\n",
    "        \"\"\"Predict category by name norm.\"\"\"\n",
    "        \n",
    "        text = self.bpe_model.encode(name_norm)\n",
    "        text = torch.tensor(text).to(self.device)\n",
    "        output = self.model(text, torch.tensor([0]).to(self.device))\n",
    "        return self.categories[output.argmax(1).item()]\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Хлеб, сладости, снеки'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Usage\n",
    "\n",
    "params = {\n",
    "    'num_class': 21,\n",
    "    'embed_dim': 50,\n",
    "    'vocab_size': 500\n",
    "}\n",
    "\n",
    "predictor = PredictCategory('receipt_parser/models/train_bpe_model.yttm', 'receipt_parser/models/baseline_model.pth', params)\n",
    "predictor.predict('джем')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
